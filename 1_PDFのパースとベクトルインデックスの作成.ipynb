{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed683b8-2e12-4be2-9921-9871cee815f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 必要なライブラリのインストール\n",
    "%pip install -U databricks-sdk databricks-vectorsearch\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63ab8608-3610-4d20-9e5d-b10984dd2b70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Databricks RAGアプリチュートリアル - 1. PDFのパースとベクトルインデックスの作成\n",
    "\n",
    "このノートブックでは、PDFをパースしてVector Search Indexに登録し、検索機能をテストするまでの一連の流れを学習します。\n",
    "\n",
    "## このワークショップで学習する内容\n",
    "1. パラメータの設定\n",
    "2. PDFのパース、前処理、Unity Catalogテーブル登録 \n",
    "3. Vector Search Indexの作成\n",
    "4. Vector Searchのテスト\n",
    "\n",
    "## 前提条件\n",
    "\n",
    "ワークショップ共通の動作条件はREADME.mdをご確認ください。\n",
    "\n",
    "- 動作確認済の環境：Serverless Notebook（環境バージョン：3）\n",
    "- `ai_parse_document`関数のプレビューが有効化されていること。実行前に、ワークスペース管理者より `Mosaic AI Agent Bricks Preview` が有効化されているかを確認してください。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccdfb254-76ee-4a83-9d83-510bbf7fccaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1-1. パラメータの設定\n",
    "\n",
    "まず、このワークショップで使用するカタログ、スキーマ、その他の設定パラメータを定義します。\n",
    "\n",
    "### カタログとスキーマについて\n",
    "- **カタログ**: Unity Catalogの最上位レベルのコンテナ。データベース、テーブル、関数などを管理\n",
    "- **スキーマ**: カタログ内のデータベース。テーブルやビューを格納\n",
    "\n",
    "既存のカタログを使用し、スキーマは既存のものを利用するか新規作成が可能です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9c3109-e579-42f9-a7de-54f7001dd7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# パラメータ設定\n",
    "CATALOG_NAME = \"skato\"  \n",
    "SCHEMA_NAME = \"rag_workshop\"  \n",
    "VOLUME_NAME = \"pdf_files\"  \n",
    "\n",
    "# Vector Search関連パラメータ\n",
    "VECTOR_SEARCH_ENDPOINT = \"one-env-shared-endpoint-1\"  \n",
    "VECTOR_INDEX_NAME = \"chunked_document_vs_index\"  \n",
    "EMBEDDING_MODEL_ENDPOINT = \"skato-plamo-embedding\"  \n",
    "\n",
    "print(f\"カタログ: {CATALOG_NAME}\")\n",
    "print(f\"スキーマ: {SCHEMA_NAME}\")\n",
    "print(f\"Vector Search Endpoint: {VECTOR_SEARCH_ENDPOINT}\")\n",
    "print(f\"Embeddingエンドポイント: {EMBEDDING_MODEL_ENDPOINT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "950b2d81-9a8e-4e67-94f2-c93384dedb5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Unity Catalogオブジェクトの設定\n",
    "\n",
    "USE CATALOG と USE SCHEMA を使用して、作業対象のカタログとスキーマを設定します。またrawデータを格納するボリュームを作成します\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4269f6a6-d658-4d73-bff9-6d9eeb5d4504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "# カタログとスキーマの設定\n",
    "spark.sql(f\"\"\"USE CATALOG {CATALOG_NAME}\"\"\")\n",
    "spark.sql(f\"\"\"DROP SCHEMA IF EXISTS {SCHEMA_NAME} CASCADE\"\"\")\n",
    "spark.sql(f\"\"\"CREATE SCHEMA {SCHEMA_NAME}\"\"\")\n",
    "spark.sql(f\"\"\"USE SCHEMA {SCHEMA_NAME}\"\"\")\n",
    "spark.sql(f\"\"\"CREATE VOLUME {VOLUME_NAME}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da47de74-bf4a-436c-b7f6-824f0eea23ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ファイルをボリュームに配置\n",
    "\n",
    "ワークショップで扱うpdfファイルをボリュームに移管します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c9465c-1e5f-4ccb-9aa1-c535f9e1308e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CATALOG_NAME\"] = CATALOG_NAME\n",
    "os.environ[\"SCHEMA_NAME\"] = SCHEMA_NAME\n",
    "os.environ[\"VOLUME_NAME\"] = VOLUME_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f0e1e75-c402-449a-a1b8-7c01e4689b2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh cp input/* /Volumes/$CATALOG_NAME/$SCHEMA_NAME/$VOLUME_NAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd196b3e-7c5d-4661-b3dc-731d239972cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1-2. PDFのパース、前処理、Unity Catalogテーブル登録\n",
    "\n",
    "### ai_parse_document関数について\n",
    "\n",
    "`ai_parse_document`は、DatabricksのAI関数の一つで、構造化されていないドキュメント（PDF、Word、PowerPointなど）を解析してテキストやメタデータを抽出する関数です。\n",
    "\n",
    "⚠️：2025年7月時点でベータ版の機能となります。実行前に、ワークスペース管理者より `Mosaic AI Agent Bricks Preview` が有効化されているかを確認してください\n",
    "\n",
    "**主な特徴:**\n",
    "- PDFからテキスト、テーブル、画像を抽出\n",
    "- ページ番号、座標などのメタデータも取得可能\n",
    "- SQLから直接実行可能\n",
    "- Unity Catalogと連携してファイルを処理\n",
    "\n",
    "**構文:**\n",
    "```sql\n",
    "ai_parse_document(file_path, [options])\n",
    "```\n",
    "\n",
    "### ai_parse_document関数を使用したPDFのパース\n",
    "\n",
    "既にVolumeに格納されているPDFファイルをパースし、Unity Catalogテーブルに保存します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c04bdf-a354-4d71-8a80-2255f54438b1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754272470286}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 複数PDFをパースしてテーブルに保存\n",
    "CREATE OR REPLACE TABLE parsed_documents AS\n",
    "SELECT \n",
    "  path,\n",
    "  ai_parse_document(content) as content -- ai_parse_document: PDF, 画像の内容をLLMでパースする\n",
    "FROM READ_FILES('/Volumes/skato/rag_workshop/pdf_files/*.pdf', format => 'binaryFile');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a8d79fb-f67e-4044-b65f-ea01917fbb52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### パース結果の確認\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd5f9fbc-1310-4ac1-992d-2c3375a0a62c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":150},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754210634724}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  path, \n",
    "  content\n",
    "FROM parsed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce03253-e0ed-453b-a80d-61b8bed93d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 要素別にドキュメントを整理（pathも保存）\n",
    "CREATE OR REPLACE TABLE parsed_documents_by_element AS (\n",
    "SELECT \n",
    "  path,\n",
    "  CAST(value:id AS INT) AS element_id,\n",
    "  CAST(value:page_id AS INT) AS page_id,\n",
    "  CAST(value:type AS STRING) AS content_type,\n",
    "  CAST(value:content AS STRING) AS content,\n",
    "  value\n",
    "FROM parsed_documents,\n",
    "LATERAL variant_explode(content:document.elements)\n",
    ");\n",
    "SELECT * FROM parsed_documents_by_element;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab70b97f-e526-4b15-b14a-18bf750b1c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 参考：ai_queryによるバッチ推論\n",
    "\n",
    "`ai_query`は、DatabricksのAI関数の一つで、自然言語の質問や指示をSQLから直接AIモデルに投げて、テキスト生成や要約、分類など様々なタスクを実行できる関数です。\n",
    "\n",
    "**主な特徴:**\n",
    "- SQLから直接AIモデルを呼び出し、自然言語処理タスクを実行可能\n",
    "- テキスト生成、要約、分類、翻訳など幅広い用途に対応\n",
    "- 入力プロンプトや追加パラメータを柔軟に指定できる\n",
    "- Unity Catalogテーブルのデータと組み合わせて活用可能\n",
    "\n",
    "**例:**\n",
    "\n",
    "```sql\n",
    "SELECT ai_query('次のレビューを要約してください: ' || review_text) AS summary FROM reviews\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16a5485c-3760-4c60-aa10-994d26178deb",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"content\":815},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754271717180}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  content\n",
    "  , ai_query('databricks-claude-sonnet-4', '以下のドキュメントを2-3文の日本語で要約して: ' || content ) as summarized_content -- ai_queryでエンドポイント（Claude Sonnet 4）とプロンプトを指定し各レコードに対して推論\n",
    "FROM\n",
    "  parsed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d178b537-23b4-4058-a0b3-211d1f2af42d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### チャンキング処理\n",
    "\n",
    "抽出されたテキストを適切なサイズのチャンクに分割します。\n",
    "Vector Searchでの検索精度を向上させるため、テキストを適切なサイズに分割し、オーバーラップを設定します。\n",
    "\n",
    "### チャンキング戦略について\n",
    "長いドキュメントは、検索とRAGの品質向上のために適切なサイズのチャンクに分割する必要があります。\n",
    "このワークショップでは、一定の要素を重複（オーバーラップ）させながら要素をチャンクさせるカスタム関数を使用します\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed49d22a-1766-4555-a741-2d3adf6c2caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"parsed_documents_by_element\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df770e0-1ffa-410d-9d16-be1ce77ecbbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def chunk_content(df: DataFrame, \n",
    "                 chunk_size: int = 5, \n",
    "                 overlap: int = 1,\n",
    "                 key_col: str = \"element_id\",\n",
    "                 content_col: str = \"content\",\n",
    "                 path_col: str = \"path\",\n",
    "                 page_col: str = \"page_id\",\n",
    "                 min_content_length: int = 0,\n",
    "                 show_stats: bool = True) -> DataFrame:\n",
    "    \"\"\"\n",
    "    複数ファイル対応：Spark DataFrameのcontentカラムをpathごとに分けてキー列の順序に基づいてチャンキングする関数\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        入力のSpark DataFrame（path, element_id, page_id, contentカラムを含む）\n",
    "    chunk_size : int, default=5\n",
    "        1つのチャンクに含める行数\n",
    "    overlap : int, default=1\n",
    "        チャンク間でオーバーラップする行数\n",
    "    key_col : str, default=\"element_id\"\n",
    "        チャンキングのキーとなる列名\n",
    "    content_col : str, default=\"content\"\n",
    "        チャンキング対象のコンテンツカラム名\n",
    "    path_col : str, default=\"path\"\n",
    "        ファイルパス列名\n",
    "    page_col : str, default=\"page_id\"\n",
    "        ページID列名\n",
    "    min_content_length : int, default=0\n",
    "        対象列の最小文字数しきい値（これ以下の行は除外）\n",
    "    show_stats : bool, default=True\n",
    "        統計情報を表示するかどうか\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        チャンキングされたDataFrame\n",
    "        カラム: path, chunk_id, chunk_content, key_ids, chunk_start_key, chunk_end_key, \n",
    "               chunk_length, chunk_start_page, chunk_end_page\n",
    "    \"\"\"\n",
    "    \n",
    "    # 入力データの行数をカウント\n",
    "    original_count = df.count()\n",
    "    \n",
    "    # 文字数フィルタリング（指定したしきい値より大きい行のみ保持）\n",
    "    df_filtered = df.filter(length(col(content_col)) > min_content_length)\n",
    "    \n",
    "    # フィルタリング後の行数をカウント\n",
    "    filtered_count = df_filtered.count()\n",
    "    removed_count = original_count - filtered_count\n",
    "    \n",
    "    # pathごとに処理を実行\n",
    "    paths = [row[path_col] for row in df_filtered.select(path_col).distinct().collect()]\n",
    "    \n",
    "    result_dfs = []\n",
    "    total_chunks = 0\n",
    "    \n",
    "    for path in paths:\n",
    "        # pathでフィルタリング\n",
    "        path_df = df_filtered.filter(col(path_col) == path)\n",
    "        \n",
    "        # キー列でソートして位置番号を振る（1から開始）\n",
    "        window_spec = Window.orderBy(key_col)\n",
    "        df_sorted = path_df.withColumn(\"position\", row_number().over(window_spec))\n",
    "        \n",
    "        # step_sizeを計算\n",
    "        step_size = chunk_size - overlap\n",
    "        \n",
    "        # 各行が属するチャンクIDの範囲を計算\n",
    "        df_with_chunk_range = df_sorted.withColumn(\n",
    "            \"min_chunk_id\",\n",
    "            greatest(lit(0), ceil((col(\"position\") - chunk_size) / lit(step_size)).cast(\"int\"))\n",
    "        ).withColumn(\n",
    "            \"max_chunk_id\", \n",
    "            floor((col(\"position\") - 1) / lit(step_size)).cast(\"int\")\n",
    "        )\n",
    "        \n",
    "        # チャンクIDのシーケンスを生成して展開\n",
    "        df_expanded = df_with_chunk_range.withColumn(\n",
    "            \"local_chunk_id\",\n",
    "            explode(sequence(col(\"min_chunk_id\"), col(\"max_chunk_id\")))\n",
    "        )\n",
    "        \n",
    "        # 実際にその行がそのチャンクに含まれるかをチェック\n",
    "        df_filtered_chunks = df_expanded.filter(\n",
    "            (col(\"position\") >= col(\"local_chunk_id\") * lit(step_size) + 1) &\n",
    "            (col(\"position\") <= col(\"local_chunk_id\") * lit(step_size) + lit(chunk_size))\n",
    "        )\n",
    "        \n",
    "        # チャンクごとにcontentを結合\n",
    "        path_result_df = df_filtered_chunks.groupBy(\"local_chunk_id\").agg(\n",
    "            # path情報を保持\n",
    "            first(col(path_col)).alias(path_col),\n",
    "            \n",
    "            # キー列でソートしてからcontentを結合\n",
    "            concat_ws(\"\\n\", \n",
    "                array_sort(\n",
    "                    collect_list(\n",
    "                        struct(col(key_col).alias(\"sort_key\"), col(content_col).alias(\"content\"))\n",
    "                    )\n",
    "                ).getField(\"content\")\n",
    "            ).alias(\"chunk_content\"),\n",
    "            \n",
    "            # キー列のリスト（ソート済み）\n",
    "            array_sort(collect_list(col(key_col))).alias(\"key_ids\"),\n",
    "                        \n",
    "            # チャンクの範囲情報\n",
    "            min(col(key_col)).alias(\"chunk_start_key\"),\n",
    "            max(col(key_col)).alias(\"chunk_end_key\"),\n",
    "            min(col(page_col)).alias(\"chunk_start_page\"),\n",
    "            max(col(page_col)).alias(\"chunk_end_page\")\n",
    "        ).withColumn(\n",
    "            \"chunk_length\", length(col(\"chunk_content\"))\n",
    "        ).withColumn(\n",
    "            # グローバルなchunk_idを生成（path名をハッシュ化 + ローカルchunk_id）\n",
    "            \"chunk_id\", \n",
    "            concat(\n",
    "                lit(f\"{paths.index(path):03d}_\"),\n",
    "                lpad(col(\"local_chunk_id\").cast(\"string\"), 4, \"0\")\n",
    "            )\n",
    "        ).drop(\"local_chunk_id\").orderBy(\"chunk_id\")\n",
    "        \n",
    "        result_dfs.append(path_result_df)\n",
    "        total_chunks += path_result_df.count()\n",
    "    \n",
    "    # 全てのpathの結果を結合\n",
    "    if result_dfs:\n",
    "        result_df = result_dfs[0]\n",
    "        for df in result_dfs[1:]:\n",
    "            result_df = result_df.unionByName(df)\n",
    "        result_df = result_df.orderBy(\"chunk_id\")\n",
    "    else:\n",
    "        # 空のDataFrameを作成\n",
    "        schema = StructType([\n",
    "            StructField(path_col, StringType(), True),\n",
    "            StructField(\"chunk_id\", StringType(), True),\n",
    "            StructField(\"chunk_content\", StringType(), True),\n",
    "            StructField(\"key_ids\", ArrayType(IntegerType()), True),\n",
    "            StructField(\"chunk_start_key\", IntegerType(), True),\n",
    "            StructField(\"chunk_end_key\", IntegerType(), True),\n",
    "            StructField(\"chunk_start_page\", IntegerType(), True),\n",
    "            StructField(\"chunk_end_page\", IntegerType(), True),\n",
    "            StructField(\"chunk_length\", IntegerType(), True)\n",
    "        ])\n",
    "        result_df = spark.createDataFrame([], schema)\n",
    "    \n",
    "    # 統計情報の表示\n",
    "    if show_stats:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"複数ファイル対応チャンキング統計情報\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 基本情報\n",
    "        print(f\"処理対象ファイル数: {len(paths)}\")\n",
    "        print(f\"処理ファイル:\")\n",
    "        for i, path in enumerate(paths):\n",
    "            print(f\"  {i+1:2d}. {path}\")\n",
    "        print(f\"キー列名: {key_col}\")\n",
    "        print(f\"対象列名: {content_col}\")\n",
    "        print(f\"チャンクサイズ: {chunk_size}\")\n",
    "        print(f\"オーバーラップサイズ: {overlap}\")\n",
    "        print(f\"ステップ数: {chunk_size - overlap}\")\n",
    "        print(f\"期待重複率: {overlap / chunk_size * 100:.1f}%\")\n",
    "        print(f\"最小文字数しきい値: {min_content_length}\")\n",
    "        \n",
    "        # データフィルタリング情報\n",
    "        print(f\"処理前総行数: {original_count:,}\")\n",
    "        print(f\"処理後総行数: {filtered_count:,}\")\n",
    "        print(f\"取り除いた行数: {removed_count:,}\")\n",
    "        if original_count > 0:\n",
    "            print(f\"除外率: {removed_count / original_count * 100:.1f}%\")\n",
    "        \n",
    "        # チャンク文字数統計を計算\n",
    "        if result_df.count() > 0:\n",
    "            stats = result_df.agg(\n",
    "                max(\"chunk_length\").alias(\"max_length\"),\n",
    "                min(\"chunk_length\").alias(\"min_length\"),\n",
    "                avg(\"chunk_length\").alias(\"avg_length\"),\n",
    "                count(\"chunk_id\").alias(\"total_chunks\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            print(f\"最大チャンク文字数: {stats['max_length']:,}\")\n",
    "            print(f\"最小チャンク文字数: {stats['min_length']:,}\")\n",
    "            print(f\"平均チャンク文字数: {stats['avg_length']:,.1f}\")\n",
    "            print(f\"総チャンク数: {stats['total_chunks']}\")\n",
    "            \n",
    "            # ファイル別統計\n",
    "            print(\"\\nファイル別チャンク数:\")\n",
    "            file_stats = result_df.groupBy(path_col).agg(\n",
    "                count(\"chunk_id\").alias(\"chunk_count\")\n",
    "            ).orderBy(path_col).collect()\n",
    "            \n",
    "            for stat in file_stats:\n",
    "                print(f\"  {stat[path_col]}: {stat['chunk_count']} チャンク\")\n",
    "        else:\n",
    "            print(\"最大チャンク文字数: 0\")\n",
    "            print(\"最小チャンク文字数: 0\")\n",
    "            print(\"平均チャンク文字数: 0.0\")\n",
    "            print(\"総チャンク数: 0\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8db3ea-0061-439a-a7ce-12be39ee6dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunked_df = chunk_content(df, chunk_size=5, overlap=1, key_col=\"element_id\", content_col=\"content\", min_content_length=0)\n",
    "\n",
    "chunked_df.write.mode(\"overwrite\").saveAsTable(\"chunked_documents\")\n",
    "display(chunked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6fe0bd1-08c2-4af0-a74a-65b7903512a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Vector Search Indexを作成するためにchange data feedを有効化\n",
    "ALTER TABLE chunked_documents SET TBLPROPERTIES (delta.enableChangeDataFeed = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b53c7b03-353f-4231-b4f0-1e59f6bea5e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1-3. Vector Search Indexの作成\n",
    "\n",
    "### Mosaic AI Vector Searchについて\n",
    "\n",
    "Mosaic AI Vector Searchは、Databricksの提供するベクトル検索サービスです。テキストや画像などの非構造化データを高次元ベクトルに変換し、類似度検索を高速に実行できます。\n",
    "\n",
    "**主な特徴:**\n",
    "- Unity Catalogとの完全統合\n",
    "- 自動的なベクトル化（埋め込み生成）\n",
    "- リアルタイムでの増分更新\n",
    "- スケーラブルな検索性能\n",
    "\n",
    "### 参考：UIでのVector Search Index作成手順\n",
    "\n",
    "#### 1. エンドポイントの作成\n",
    "\n",
    "1. 左サイドバーで**「Compute」**をクリック。\n",
    "2. 上部の**「Vector Search」**タブを選択し、**「Create」**ボタンを押す。\n",
    "3. 「Create endpoint」フォームが開くので以下を入力・設定する\n",
    "   - **エンドポイント名**を入力\n",
    "   - **Type**フィールドで「Standard」または「Storage Optimized」を選択\n",
    "   - （必要な場合は）Advanced settings で予算ポリシーを設定\n",
    "4. **「Confirm」**をクリックして作成完了\n",
    "\n",
    "#### 2. インデックスの作成\n",
    "\n",
    "1. 左サイドバーで**「Catalog」**をクリックしてCatalog Explorerを開く。\n",
    "2. 対象とする**Deltaテーブル**を選択。\n",
    "3. 右上の**「作成」**ボタン、もしくはケバブ（三点）メニューから**「Vector search index」**を選択。\n",
    "4. 「Create vector index」フォームで以下を設定\n",
    "   - **名前**: `<catalog>.<schema>.<name>` 形式で入力（英数字＋アンダースコアのみ）\n",
    "   - **主キー**: 主キーとなるテーブルのカラムを指定\n",
    "   - **エンドポイント**: 先ほど作成したエンドポイントを選択\n",
    "   - **同期する列**: 標準エンドポイントの場合のみ必要、同期対象の列を選択（空欄なら全カラム同期）\n",
    "   - **埋め込みソース（Embedding source）**:\n",
    "     - Databricksにテキスト列から埋め込み計算させる場合は「Compute embeddings」を選択\n",
    "     - 既存の埋め込みベクトルカラムを使う場合は「Use existing embedding column」を選択\n",
    "   - **埋め込みモデルエンドポイント**（必要に応じて）を選択\n",
    "5. 設定後、**「Confirm」**で作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25ea6c4a-f3a4-451c-8f35-14c079ad647f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Python SDKでのIndex実装\n",
    "\n",
    "以下では、Python SDKを使用してプログラマティックにVector Search Indexを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b172dca-6e0f-427d-9305-cef82687a610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "import time\n",
    "\n",
    "# Vector Search Clientの初期化\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "print(\"Vector Search Clientを初期化しました\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7189d6e1-51cb-4ba2-bb5c-8d74ada6445b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Vector Search Endpointの作成または確認\n",
    "\n",
    "Vector Search Endpointは、Vector Searchサービスにアクセスするためのエンドポイントです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "322db0b5-c384-4fee-900b-e1c3f3e35ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # 既存のエンドポイントを確認\n",
    "    endpoint = vsc.get_endpoint(VECTOR_SEARCH_ENDPOINT)\n",
    "    print(f\"既存のVector Search Endpoint '{VECTOR_SEARCH_ENDPOINT}' を使用します\")\n",
    "    print(f\"エンドポイントステータス: {endpoint['endpoint_status']['state']}\")\n",
    "except Exception as e:\n",
    "    print(f\"エンドポイント '{VECTOR_SEARCH_ENDPOINT}' が見つかりません。新規作成します...\")\n",
    "    \n",
    "    # 新しいエンドポイントを作成\n",
    "    endpoint = vsc.create_endpoint(\n",
    "        name=VECTOR_SEARCH_ENDPOINT,\n",
    "        endpoint_type=\"STANDARD\"\n",
    "    )\n",
    "    \n",
    "    # エンドポイントの準備完了を待機\n",
    "    while endpoint.endpoint_status.state == \"PROVISIONING\":\n",
    "        print(\"エンドポイントの準備中...\")\n",
    "        time.sleep(30)\n",
    "        endpoint = vsc.get_endpoint(VECTOR_SEARCH_ENDPOINT)\n",
    "    \n",
    "    print(f\"Vector Search Endpoint '{VECTOR_SEARCH_ENDPOINT}' を作成しました\")\n",
    "    print(f\"エンドポイントステータス: {endpoint.endpoint_status.state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1914f08e-1b20-4566-b773-5557fa4f58a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Vector Search Indexの作成\n",
    "\n",
    "チャンキング済みのドキュメントテーブルに対してVector Search Indexを作成します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec1467b-a5df-4c09-99cb-aaaf11f50cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 完全なテーブル名とインデックス名を定義\n",
    "source_table_fullname = f\"{CATALOG_NAME}.{SCHEMA_NAME}.chunked_documents\"\n",
    "vs_index_fullname = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{VECTOR_INDEX_NAME}\"\n",
    "\n",
    "print(f\"ソーステーブル: {source_table_fullname}\")\n",
    "print(f\"インデックス名: {vs_index_fullname}\")\n",
    "print(f\"Embeddingエンドポイント: {EMBEDDING_MODEL_ENDPOINT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da572eb1-54b1-48bc-ad74-fad9f49f1dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # 既存のインデックスを確認\n",
    "    index = vsc.get_index(vs_index_fullname)\n",
    "    print(f\"既存のVector Search Index '{vs_index_fullname}' が見つかりました\")\n",
    "    print(f\"インデックスステータス: {index.status.ready}\")\n",
    "except Exception as e:\n",
    "    print(f\"インデックス '{vs_index_fullname}' が見つかりません。新規作成します...\")\n",
    "    \n",
    "    # 新しいインデックスを作成\n",
    "    index = vsc.create_delta_sync_index(\n",
    "        endpoint_name=VECTOR_SEARCH_ENDPOINT,\n",
    "        index_name=vs_index_fullname,\n",
    "        source_table_name=source_table_fullname,\n",
    "        primary_key=\"chunk_id\",\n",
    "        pipeline_type=\"TRIGGERED\",\n",
    "        embedding_source_column=\"chunk_content\",\n",
    "        embedding_model_endpoint_name=EMBEDDING_MODEL_ENDPOINT\n",
    "    )\n",
    "   \n",
    "    print(f\"Vector Search Index '{vs_index_fullname}' の作成を開始しました\")\n",
    "    print(\"インデックスの構築には数分かかる場合があります...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d679430e-5e27-4cac-a6c4-c66a3d24e994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### インデックス作成状況の確認\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995b4311-38ba-48c4-87ff-40438aea9f17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "index.wait_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19483f4b-ac6a-466c-b4b9-b4307cb76252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1-4. Vector Searchのテスト\n",
    "\n",
    "作成したVector Search Indexを使用して、実際に類似度検索を実行してみます。\n",
    "\n",
    "### Python SDKによる検索テスト\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87493b9e-482a-4e13-b143-d17886fafadd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# テスト用のクエリ\n",
    "test_query = \"生成AIの開発ワークフローについて教えてください\"\n",
    "\n",
    "print(f\"検索クエリ: {test_query}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Vector Searchによる類似度検索\n",
    "results = vsc.get_index(index_name=vs_index_fullname).similarity_search(\n",
    "    query_text=test_query,\n",
    "    columns=[\"chunk_id\", \"chunk_content\"],\n",
    "    num_results=5\n",
    ")\n",
    "\n",
    "print(f\"検索結果: {len(results['result']['data_array'])} 件\")\n",
    "print(\"\\n--- 検索結果 ---\")\n",
    "\n",
    "for i, result in enumerate(results['result']['data_array'], 1):\n",
    "    chunk_id = result[0]\n",
    "    content = result[1]\n",
    "    \n",
    "    print(f\"\\n{i}. チャンクID: {chunk_id}\")\n",
    "    print(f\"   内容: {content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8623e4c1-fb9a-4d0b-8bec-64db48b40ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### SQLのvector_search関数による検索テスト\n",
    "\n",
    "DatabricksのSQL環境では、`vector_search`関数を使用してVector Searchを実行できます。これにより、SQLクエリ内で直接ベクトル検索を組み込むことができます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19370af9-1d22-460c-9319-38fcaf9e3565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQLのvector_search関数を使用した検索\n",
    "SELECT \n",
    "  *\n",
    "FROM vector_search(\n",
    "  index => 'skato.rag_workshop.chunked_document_vs_index',\n",
    "  query => '生成AIの開発ワークフローについて教えてください',\n",
    "  num_results => 5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04273944-4fe1-4ae7-9540-a8a9922bff8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## まとめ\n",
    "\n",
    "このワークショップでは、以下の内容を学習しました：\n",
    "\n",
    "1. **パラメータ設定**: Unity Catalogのカタログとスキーマの設定\n",
    "2. **PDFパース**: `ai_parse_document`関数を使用したPDFの解析とチャンキング\n",
    "3. **Vector Search Index作成**: Mosaic AI Vector Searchのセットアップ\n",
    "4. **検索テスト**: Python SDKとSQLによる類似度検索の実行\n",
    "\n",
    "### 次のステップ\n",
    "次は、このVector Search Indexを活用してRAG（Retrieval-Augmented Generation）アプリケーションを構築します。\n",
    "\n",
    "### 参考リンク\n",
    "- [Databricks AI Functions](https://docs.databricks.com/aws/ja/sql/language-manual/functions/ai_parse_document)\n",
    "- [Mosaic AI Vector Search](https://docs.databricks.com/aws/en/generative-ai/create-query-vector-search)\n",
    "- [RAGデータパイプラインのベストプラクティス](https://docs.databricks.com/aws/en/generative-ai/tutorials/ai-cookbook/quality-data-pipeline-rag)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6398222385249119,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "1_PDFのパースとベクトルインデックスの作成",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
